---
layout: single
title: "Kosmos-1 (2023-02-28)"
categories: [paper]
tag: [Artificial General Intelligence, Multimodal Large Language Model, Zero-shot Learning, Few-shot Learning, Cross-modal Transfer]
toc: true
toc_sticky: true
toc_label: Contents
---

# Language Is Not All You Need: Aligning Perception with Language Models
[arxiv](https://arxiv.org/pdf/2302.14045.pdf), [github](https://github.com/microsoft/unilm#llm--mllm-multimodal-llm)
- Kosmos-1 is a Multimodal Large Language Model (MLLM) that can perceive various modalities and follow instructions without finetuning. It achieves impressive performance on language understanding, perception-language tasks, and vision tasks. MLLMs can benefit from cross-modal transfer, and a new dataset of Raven IQ tests is introduced to diagnose nonverbal reasoning capability.

1️⃣ The underlying assumption of this paper is that a key step toward artificial general intelligence is the convergence of language, multimodal perception, action, and world modeling. The authors also assume that a Multimodal Large Language Model (MLLM) can learn to perceive general modalities, learn in context (i.e., few-shot), and follow instructions (i.e., zero-shot) without any gradient updates or fine-tuning.

2️⃣ The methodology used in this paper involves training a new MLLM called Kosmos-1 from scratch on web-scale multimodal corpora, including text, images, and image-caption pairs. The model is evaluated on various tasks, including language understanding, generation, and OCR-free NLP, as well as perception-language tasks such as multimodal dialogue, image captioning, and visual question answering. The authors also introduce a new dataset for diagnosing the nonverbal reasoning capability of MLLMs.

3️⃣ The implications of this research are that MLLMs can achieve impressive performance on a wide range of tasks, including perception-language tasks and vision tasks, without requiring fine-tuning or gradient updates. The authors also demonstrate that MLLMs can benefit from cross-modal transfer, which has potential implications for artificial general intelligence.

**The Big Convergence**
- Large-scale self-supervised pre-training across tasks (predictive and generative), languages (100+ languages), and modalities (language, image, audio, layout/format + language, vision + language, audio + language, etc.)

## A Categorical Framework of General Intelligence

> We assume that Fθ can accurately understand multi-modal signals from the sensor with the help of multi-modality learning, like e.g., CLIP (Radford et al., 2021), Dalle2 (Ramesh et al., 2022), EVA (Fang et al., 2022), Kosmos-1 (Huang et al., 2023), etc. In particular, both the image of a dog and the text describing a dog, will be mapped to the same representation of a dog, using contrastive learning techniques with InfoNCE loss Radford et al. (2021); Chen et al. (2020).

