---
layout: single
title: "Kosmos-1 (2023-02-28)"
categories: [paper]
tag: [Artificial General Intelligence, Multimodal Large Language Model, Zero-shot Learning, Few-shot Learning, Cross-modal Transfer]
toc: true
toc_sticky: true
toc_label: Contents
---

# Language Is Not All You Need: Aligning Perception with Language Models
[arxiv](https://arxiv.org/pdf/2302.14045.pdf), [github](https://github.com/microsoft/unilm#llm--mllm-multimodal-llm)
- Kosmos-1 is a Multimodal Large Language Model (MLLM) that can perceive various modalities and follow instructions without finetuning. It achieves impressive performance on language understanding, perception-language tasks, and vision tasks. MLLMs can benefit from cross-modal transfer, and a new dataset of Raven IQ tests is introduced to diagnose nonverbal reasoning capability.

**The Big Convergence**
- Large-scale self-supervised pre-training across tasks (predictive and generative), languages (100+ languages), and modalities (language, image, audio, layout/format + language, vision + language, audio + language, etc.)

## A Categorical Framework of General Intelligence

> We assume that FÎ¸ can accurately understand multi-modal signals from the sensor with the help of multi-modality learning, like e.g., CLIP (Radford et al., 2021), Dalle2 (Ramesh et al., 2022), EVA (Fang et al., 2022), Kosmos-1 (Huang et al., 2023), etc. In particular, both the image of a dog and the text describing a dog, will be mapped to the same representation of a dog, using contrastive learning techniques with InfoNCE loss Radford et al. (2021); Chen et al. (2020).

