---
layout: single
title: "2028-03-01 논문 리뷰"
categories: papers
tags: [LLM, paper]
toc: true
toc_sticky: true
toc_label: 목차
---

### 1. A Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT
- Pretrained Foundation Model(PFM)이란, BERT, GPT-3, MAE, DALLE-E, ChatGPT와 같이 다양한 데이터 모달리티에서 사용이 되는 기초 모델입니다.
- 대규모 데이터를 통해서 사전 학습이 되고 이를 응용해서 다양한 분야에서 좋은 성능을 보입니다.
- 기존의 CNN, RNN이 아니라 Transformer을 통해 특징을 추출하고, self-supervised learning을 진행합니다.
- 최근 ChatGPT와 같은 대규모 언어 모델에서는 zero-shot, few-shot 학습을 통해 높은 성능을 보입니다.
- 이 논문에서는 텍스트, 이미지, 그래프 등 다양한 데이터에서의 PFM 연구 및 도전 과제를 다룹니다.
- 모델의 효율성 및 압축, 보안 및 개인정보 보호와 같은 이슈를 확인할 수 있습니다.

### 2. More than you've asked for: A Comprehensive Analysis of Novel Prompt Injection Threats to Application-Integrated Large Language Models
- 요즘 계속해서 엄청난 크기의 언어 모델이 공개되고 있는데요, 개발 환경이나 검색 엔진과 같이 다양한 서비스를 제공하고 있습니다.
- 이제는 기본적으로 자연어 프롬프트로 다양한 작업을 지시하게 되는 세상이 왔습니다.
- 그런데 새로운 서비스가 공개될 때마다 Prompt Injection 공격 방식으로 기존의 LLM 모델에 설계된 동작 방식을 우회하고 있습니다.
- 현재까지는 주로 프롬프트를 통해 공격을 했다면, 이번 논문에서는 Application-integrated LLMs이라는 새로운 모델에서 발생하는 위협에 대해 연구한 것입니다.
- 이 모델들은 검색이나 API 호출 기능을 통해 악성 프롬프트가 미리 삽입된 해로운 컨텐츠를 처리할 수 있어요.
- 이런 새로운 공격 가능성에 대해 연구했고, 이에 따른 새로운 방어 기술도 연구해야 한다는 결론을 내리고 있네요.

### 3. EvoPrompting: Language Models for Code-Level Neural Architecture Search
- 그동안 딥러닝 모델을 어떻게 하면 더욱 발전시킬 수 있을 것인가에 대한 고민을 모두가 해왔을텐데요.
- 이 논문에서는 언어 모델을 통해서 진화적 NAS 알고리즘을 수행하는 새로운 방법을 제안합니다.
- 여기서는 EvoPrompting이라고 부르는데요, NAS를 완전히 대체 하지는 않지만, 진화적인 프롬프트 엔지니어링과 soft prompt-tuning을 결합해서 다양하고 성능이 좋은 모델을 찾을 수 있다고 합니다.
- MNIST-1D 데이터에 적용을 해보았더니, 이전의 기존 모델보다 우수한 결과를 도출했다고 하네요.

### 4. In-Context Instruction Learning
- 기존의 LLM에서는 Intruction learning을 통해서 zero-shot 작업을 할 수 있게 되었는데요. 그럼에도 여전히 fine-tuning과 RLHF이라는 단계가 존재했습니다.
- 그런데 이 논문에서는 새로운 접근 방식인 In-Context Instruction Learning (ICIL)을 제시합니다.
- 기존 instruction learning에 in-context learning을 적용하는 것인데요. 특히, 모든 작업에 대해 하나의 고정된 prompt를 사용하는 것이 핵심적인 장점 중 하나라고 합니다.


